**Что нужно для запуска и тестирования проекта:**
1. Клонировать репозиторий
2. Скачать все необходимые библиотеки
3. Версия python 3.11

**Для запуска и обучения в google colab:**
1. Загрузить все файлы кода в архив .zip
2. Загрузить архив на гугл диск
3. Прописать в ячейки кода в коллабе следующее:

   1) from google.colab import drive
      drive.mount('/content/drive')

   2) import os

      # Создаем папку для проекта в среде Colab
      !mkdir -p /content/checkers_ai

      # Копируем архив с Диска и распаковываем
      !cp /content/drive/MyDrive/CheckersAI/checkers_project.zip /content/checkers_ai/
      !unzip -o /content/checkers_ai/checkers_project.zip -d /content/checkers_ai/

      # Переходим в папку с проектом
      os.chdir('/content/checkers_ai')

      print("Файлы распакованы, рабочая директория:", os.getcwd())

   3) import os

      # 1. Заходим внутрь вложенной папки
      os.chdir('/content/checkers_ai/checkers_project')

      # 2. Проверяем, что мы на месте
      print("Текущая папка:", os.getcwd())
      print("Файлы здесь:", os.listdir())
      
      # 3. Запускаем!
      !python main.py
   
**Пояснения к структуре проекта:**
1. В папке **tars** находятся архивы с весами нейросети на разных итерациях,
   полученные в ходе экспериментов обучения.
2. В папке **tests** находятся тесты для проверки корректности логики правил игры и архитектуры сети
   (сейчас она обновлена, используется ResNet)
3. **cleaner.py** очищает файлы истории обучения модели от метаданных, оставляет только веса
   (на гите лежат уже очищенные)
4. **compare_models.py** сравнивает характеристики обученных моделей
   (пока можно только сравнивать модели, обученные в ходе первого эксперимента,
   в результате видим количество итераций, количество примеров игр,
   скорость обучения на момент обучения, среднее значение весов первого слоя)
5. **config.py** хранит все параметры для обучения:
       название эксперимента,
       параметры доски,
       вероятности каждого возможного хода,
       количество эпох,
       размер батча,
       скорость обучения,
       количество симуляций,
       коэффициент исследования,
       количество итераций
6. **game.py** отвечает за саму логику шашек (обязательное взятие,
   взятие наибольшего количества шашек противника и тд.,)
   хранит условные обозначения:
   0 Пустая клетка, 1	Белая пешка, -1	Черная пешка, 2	Белая дамка, -2	Черная дамка
7. **inspector.py** проверяет файлы чекпоинтов, был нужен в одной из начальных версий
8. **main.py** запускает весь цикл обучения, связывая файлы config, game, network, mcts, train
9. **mcts.py** реализует всю логику алгоритма поиска по дереву Монте-Карло,
    хранит статистику дерева, добавляет шум Дирихле во избежания однообразия сценариев игр.
10. **network.py** описывает структуру нейросети, количество ее слоев.
    Используется структура ResNet по аналогии с AlphaZero.
    Так сигнал проходит через слои без искажений.
    Нейросеть рассматривает доску в виде 4 слоев следующим образом:
      Где стоят мои пешки
      Где стоят мои дамки
      Где стоят пешки врага
      Где стоят дамки врага
    На выходе мы получаем 12-13 слоев.
11. **play.py** - пользовательский интерфейс для тестирования модели.
    Можно выбрать с какой версией играть,
    рекомендую использовать файлы из папки little во втором эксперименте.
    При помощи библиотеки pygame отображается шахматная доска,
    пользователь играет за черные шашки, нейросеть за белые.
    Взятие обязательно по правилам игры.
12. **plotter.py** строит графики функций потерь (value loss и policy loss),
    а также график средней длины игр.
    Графики строятся каждые 100 итераций, а также итоговые по всем итерациям
    (есть только для первого эксперимента).
    Они сохранены в папке plots, ее я не загружала на гит.
13. **train.py** обучает нейросеть на предоставленных примерах,
    а также сохраняет чекпоинты.
14. **utils.py** необходим для передачи тензора в нейросеть из состояния доски
    (забирает данные из config.py, передает в mcts.py),
    без него возникала ошибка рекурсии.
    
    
    
